# Sorting Algorithms Benchmarking Report

## 1. Experimental Setup

This report details the methodology and environment used to benchmark various sorting algorithms.

### 1.1. Machine Specifications

*   **Operating System:** Windows
*   **Model:** ASUS Vivobook 16X
*   **Processor:** Intel i7
*   **Graphics Card:** NVIDIA RTX 4050 Studio
*   **Serial Number:** R6N0CX02K537230

### 1.2. Timing Mechanism

The execution time for each sorting algorithm was measured using **Python's `time.perf_counter()`**. This function provides a high-resolution performance counter, suitable for benchmarking short durations. The C programs were executed as subprocesses, and their total execution time (including input reading, sorting, and any internal overhead) was captured by the Python script.

### 1.3. Number of Experiment Repetitions

Each sorting experiment (a specific algorithm run on a particular input size and type) was repeated **7 times**. This was done to mitigate the impact of transient system loads, background processes, and other environmental factors that could introduce variability in single measurements.

### 1.4. Reported Times

For each experiment, the **average execution time** across the 7 repetitions is reported. Times are presented in **seconds (s)**, formatted to six decimal places for precision. In cases where an algorithm failed to complete (e.g., due to stack overflow for certain Quick Sort variants on large, pathological inputs), "Crashed/Timeout" is reported.

### 1.5. Input Selection

The test data used for benchmarking was pre-generated by the `generate_test_data.py` script. The inputs were selected to cover a range of sizes and initial orderings to evaluate best-case, worst-case, and average-case performance:

*   **Input Sizes (N):** The following input sizes were tested: 100, 500, 1000, 5000, 10000, 25000, 50000, 75000, and 100000 elements.
*   **Input Types:** For each input size, three distinct types of data were generated:
    *   **Random:** A randomly ordered array of integers. This serves to evaluate the **average-case** performance of the algorithms.
    *   **Sorted:** An array of integers already sorted in ascending order. This is used to evaluate **best-case** performance for some algorithms (e.g., Insertion Sort, Merge Sort) and **worst-case** performance for others (e.g., Quick Sort with naive pivot selection).
    *   **Reverse Sorted:** An array of integers sorted in descending order. This typically represents a **worst-case** scenario for many comparison-based sorting algorithms.

### 1.6. Consistency of Inputs

**Yes, the same set of input data files was used for all sorting algorithms.** This ensures a fair and direct comparison of their performance characteristics under identical conditions. Each C sorting program was modified to read its input from standard input (stdin), allowing the Python benchmarking script to feed the exact same data to each algorithm.

## 2. Benchmarking Results

The detailed results, including average execution times for each algorithm, input type, and input size, are presented in the generated table and plots. These are available in the `graphs/` directory:

*   `all_sorting_benchmark_results.csv`: A CSV file containing the complete tabular data.
*   `sorting_algorithms_average_case_(random_input)_case.png`: Plot showing average-case performance.
*   `sorting_algorithms_best_case_(sorted_input)_case.png`: Plot showing best-case performance.
*   `sorting_algorithms_worst_case_(reverse_sorted_input)_case.png`: Plot showing worst-case performance.
